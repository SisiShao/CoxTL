{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6ee16e",
   "metadata": {},
   "source": [
    "1. Simulate the true event time vectors for both target and source, denoted as $\\mathbf{Y}_s\\mathbf{Y}_t$: \n",
    "\n",
    "*The source model is*\n",
    "\n",
    "$$\\log y_s = \\mathbf{x}_s^T\\boldsymbol{\\omega}_s+\\sigma_s\\epsilon_s,\\mathbf{x}_s\\sim\\mathcal{N}(\\mathbf{0},I),\\epsilon_s\\sim\\mathcal{N}(0, 1)$$ \n",
    "\n",
    "*and the target model:*\n",
    "\n",
    "$$\\log y_t = \\mathbf{x}_t^T\\boldsymbol{\\beta}+\\sigma_t\\epsilon_t,\\mathbf{x}_t\\sim\\mathcal{N}(\\mathbf{0},I),\\epsilon_t\\sim\\mathcal{N}(0, 1).$$\n",
    "\n",
    "Generate 10000 $y_s$ and 1000 $y_t$ using $\\boldsymbol{\\beta},\\boldsymbol{\\omega}\\in R^{500}$ and $\\mathbf{x}_t,\\mathbf{x}_s\\in R^{500}$. Note that for each pair $(\\omega_j,\\beta_j),j=1,\\cdots,500$, we have\n",
    "\n",
    "$$(\\omega_j,\\beta_j)\\sim^{i.i.d}\\mathcal{N}\\left(0,\\frac{1}{p}\\left(\\begin{matrix}\\alpha_s^{2}&\\rho\\alpha_s\\alpha_t\\\\\\rho\\alpha_s\\alpha_t&\\alpha_t^{2}\\end{matrix}\\right)\\right)$$. Repeat the process for 10 times, then set $\\beta_j$ equalling to the average of the 10 $\\beta_js$\n",
    "\n",
    "*We consider right censoring,*\n",
    "- Assume 20% of source populations are censored and 40% of target population are censored\n",
    "- We observe $(Y_{s},\\delta_{s})\\ (Y_{t},\\delta_{t})$, where $\\delta_i,\\ i\\in\\{s,t\\}$ is the binary censoring indicator, with 1 denoting event and 0 denoting censoring.\n",
    "\n",
    "2. Split the target data into testing and training part in a ratio of 1:9, named as $X_{target\\ training}$\n",
    " and $X_{target\\ testing}$\n",
    "3. Apply the methods  (we use CoxKL in this setting; can also consider using Tian Gu's Angle TL,RF,Commute - BUT binary outcomes only) to obtain $\\widehat{\\boldsymbol{\\omega}}_1,\\cdots,\\widehat{\\boldsymbol{\\omega}}_{10}$ to estimate $\\widehat{\\beta}$, and then obtain $X_{target\\ training}\\widehat{\\beta}$ \n",
    "4. We then obtain $X_{target\\ training}\\widehat{\\boldsymbol{\\omega}}_1,\\cdots,X_{target\\ training}\\widehat{\\boldsymbol{\\omega}}_{10}$\n",
    "5. Regress $Y_{target\\ testing}$ on $X_{target\\ testing}\\widehat{\\boldsymbol{\\omega}}_1,\\cdots,X_{target\\ testing}\\widehat{\\boldsymbol{\\omega}}_{10},X_{target\\ testing}\\widehat{\\beta}$ with $L_2$ penalty.\n",
    "\n",
    "$$\\gamma=\\arg\\min \\prod_{i=1}^nf(y_i)+\\lambda\\lVert\\gamma\\lVert_2^2$$\n",
    "\n",
    "where $$f(y_i)$$ is the log-normal density and $$\\log Y_{target\\ testing}=X_{target\\ testing}\\widehat{\\boldsymbol{\\omega}}_1\\gamma_1+\\cdots+X_{target\\ testing}\\widehat{\\boldsymbol{\\omega}}_{10}\\gamma_{10}+ X_{target\\ testing}\\widehat{\\beta}\\gamma_{11}+\\boldsymbol\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7997e0",
   "metadata": {},
   "source": [
    "# Simulation Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7ae16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)  # Set a random seed for reproducibility\n",
    "\n",
    "# Set the dimensions and number of repetitions\n",
    "n_reps = 10\n",
    "n_variables = 300\n",
    "n_source = 10000\n",
    "n_target = 1000\n",
    "\n",
    "# Set the parameters\n",
    "alpha_s = 1.5\n",
    "alpha_t = 2.0\n",
    "rho = 0.5\n",
    "p = n_variables\n",
    "\n",
    "# Set the censoring proportions\n",
    "censor_prop_s = 0.2\n",
    "censor_prop_t = 0.4\n",
    "\n",
    "# Initialize the beta_j values\n",
    "beta_j_values = np.zeros(n_variables)\n",
    "\n",
    "# Loop for repetitions\n",
    "omega = {}\n",
    "beta = {}\n",
    "X_s ={}\n",
    "X_t = {}\n",
    "for rep in range(n_reps):\n",
    "    # Generate the omega_j and beta_j values\n",
    "    params = np.random.multivariate_normal(\n",
    "        mean=np.zeros(2), cov=(1/p)*np.array([[alpha_s**2, rho*alpha_s*alpha_t],\n",
    "                                              [rho*alpha_s*alpha_t, alpha_t**2]]), size=n_variables)\n",
    "    beta[rep] = params[:,1]\n",
    "    omega[rep] = params[:,0]\n",
    "    # Generate X_s and X_t\n",
    "    X_s[rep] = np.random.normal(0, 1, size=(n_source,n_variables))\n",
    "    X_t[rep] = np.random.normal(0, 1, size=(n_target,n_variables ))\n",
    "\n",
    "beta = sum(beta.values())/len(beta)\n",
    "\n",
    "Y_s = {}\n",
    "Y_t = {}\n",
    "# Generate true event times for source and target\n",
    "for rep in range(n_reps):\n",
    "    Y_s[rep] = np.exp(np.dot(X_s[rep], beta) + alpha_s * np.random.normal(0, 1, size=(n_source)))\n",
    "    Y_t[rep] = np.exp(np.dot(X_t[rep], beta) + alpha_t * np.random.normal(0, 1, size=(n_target)))\n",
    "\n",
    "# Generate censoring indicators\n",
    "delta_s = {}\n",
    "delta_t = {}\n",
    "for rep in range(n_reps):\n",
    "    delta_s[rep] = np.random.binomial(1, 1 - censor_prop_s, size=(n_source))\n",
    "    delta_t[rep] = np.random.binomial(1, 1 - censor_prop_t, size=(n_target))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb46e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y_s: (1000,)\n",
      "Shape of Y_t: (100,)\n",
      "Shape of delta_s: (1000,)\n",
      "Shape of delta_t: (100,)\n",
      "Shape of X_s: (1000, 300)\n",
      "Shape of X_t: (100, 300)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the generated data\n",
    "print(\"Shape of Y_s:\", Y_s[0].shape)\n",
    "print(\"Shape of Y_t:\", Y_t[0].shape)\n",
    "print(\"Shape of delta_s:\", delta_s[0].shape)\n",
    "print(\"Shape of delta_t:\", delta_t[0].shape)\n",
    "\n",
    "print(\"Shape of X_s:\", X_s[0].shape)\n",
    "print(\"Shape of X_t:\", X_t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afb4f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    pd.DataFrame(X_s[i]).to_csv(\"../data/source/source_X\" + str(i+1) + \".csv\", index=False)\n",
    "    pd.DataFrame(np.vstack([Y_s[i], delta_s[i]]).T).to_csv(\"../data/source/source_Y\" + str(i+1) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d2d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 90\n",
      "Testing data size: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example target data\n",
    "target_data = X_t[0]\n",
    "# Split the target data into training and testing sets\n",
    "train_data, test_data = train_test_split(target_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of training and testing data\n",
    "print(\"Training data size:\", len(train_data))\n",
    "print(\"Testing data size:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064b38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
