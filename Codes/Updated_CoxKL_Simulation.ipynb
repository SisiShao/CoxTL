{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6ee16e",
   "metadata": {},
   "source": [
    "1. Simulate the true event time vectors for both target and source, denoted as $\\mathbf{Y}_s\\mathbf{Y}_t$: \n",
    "\n",
    "*The source model is*\n",
    "\n",
    "$$y_s = \\left(\\frac{-\\log U}{\\lambda\\exp\\mathbf{x}_s^T\\boldsymbol{\\omega}_s}\\right)^{1/\\nu},U\\sim \\mathcal{U}(0,1)$$\n",
    "\n",
    "where $\\lambda$ and $\\nu$ are parameters of Weibull distributions.\n",
    "\n",
    "*The target model is:*\n",
    "\n",
    "$$y_t = \\left(\\frac{-\\log U}{\\lambda\\exp\\mathbf{x}_t^T\\boldsymbol{\\omega}_t}\\right)^{1/\\nu}.$$\n",
    "\n",
    "Generate 100 $y_s$ and 40 $y_t$  using $\\boldsymbol{\\beta},\\boldsymbol{\\omega}\\in R^{5}$ and $\\mathbf{x}_t,\\mathbf{x}_s\\in R^{5}$. Note that for each pair $(\\omega_j,\\beta_j),j=1,\\cdots,5$, we have\n",
    "\n",
    "$$(\\omega_j,\\beta_j)\\sim^{i.i.d}\\mathcal{N}\\left(1,\\left(\\begin{matrix}\\alpha_s^{2}&\\rho\\alpha_s\\alpha_t\\\\\\rho\\alpha_s\\alpha_t&\\alpha_t^{2}\\end{matrix}\\right)\\right)$$. \n",
    "\n",
    "*We consider right censoring,*\n",
    "\n",
    "- Assume 20% of source populations are censored and 40% of target population are censored, the censoring time is\n",
    "$$C_s,C_t\\sim_{iid}Weibull(\\lambda_c,\\nu_c).$$\n",
    "\n",
    "- We observe $(Y_{s},\\delta_{s})\\ (Y_{t},\\delta_{t})$, where $\\delta_i,\\ i\\in\\{s,t\\}$ is the binary censoring indicator, with 1 denoting event and 0 denoting censoring.\n",
    "<font color=\"red\">Among the five covariates, three of them are continuous, $Z_1\\sim\\mathcal{N}(1.05,0.0225), Z_2\\sim\\mathcal{N}(30,25), Z_3\\sim\\mathcal{N}(90,25)$. Two of them are discrete $Z_4,Z_5 \\sim Ber(0.5).$</font>\n",
    "\n",
    "- Creatinine is a waste product produced by the muscles and is filtered from the blood by the kidneys. It's commonly used as a marker for kidney function, and its levels in the bloodstream can indicate how well the kidneys are working.The normal range for creatinine in the blood varies by age, sex, and muscle mass. Here are the general reference ranges for serum creatinine in adults:\n",
    "    * Men: 0.74 to 1.35 mg/dL\n",
    "\n",
    "- The Urine Albumin-to-Creatinine Ratio (UACR) measures the amount of albumin in the urine compared to creatinine. It's a commonly used test to detect early kidney damage, especially in people with diabetes or hypertension.The normal range for UACR is: \n",
    "    * Less than 30 mg/g:\n",
    "    * Normal 30-299 mg/g: Moderately increased (sometimes termed \"microalbuminuria\") \n",
    "    * 300 mg/g and above: Severely increased (sometimes termed \"macroalbuminuria\")\n",
    "\n",
    "- The estimated glomerular filtration rate (eGFR) is a test used to assess how well the kidneys are functioning. It is estimated based on a formula that includes serum creatinine levels, age, gender, and sometimes other factors.The normal eGFR range varies by age, as kidney function can decrease naturally with age. In adults, the general breakdown for eGFR values is:\n",
    "\n",
    "    * eGFR >90 mL/min/1.73 m²: Normal or high function\n",
    "    * eGFR 60-89 mL/min/1.73 m²: Slightly decreased function; may be considered normal for some patients, especially the elderly.\n",
    "    * eGFR 45-59 mL/min/1.73 m²: Mildly decreased function (stage 3a chronic kidney disease, CKD)\n",
    "    * eGFR 30-44 mL/min/1.73 m²: Moderately decreased function (stage 3b CKD)\n",
    "    * eGFR 15-29 mL/min/1.73 m²: Severely decreased function (stage 4 CKD)\n",
    "    * eGFR <15 mL/min/1.73 m²: Kidney failure (stage 5 CKD or end-stage renal disease)\n",
    "\n",
    "2. Split the target data into testing and training part in a ratio of 1:1, named as $Z_{target\\ training}$\n",
    " and $Z_{target\\ testing}$\n",
    "3. Apply the methods  (we use CoxKL in this setting) to obtain $\\widehat{\\boldsymbol{\\omega}}$ to estimate $\\widehat{\\beta}$, and then obtain $Z_{target\\ testing}\\widehat{\\beta}$.\n",
    "4. Compute C-index and other measures of performances.\n",
    "5. Repeat step 3 and 4 using different values of $\\eta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7997e0",
   "metadata": {},
   "source": [
    "# Simulation Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d064b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, weibull_min, bernoulli\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Parameters\n",
    "N_s, N_t = 100, 40\n",
    "dim = 5\n",
    "alpha_s, alpha_t, rho = 1, 1, 0.5  # adjust these as needed\n",
    "cov_matrix = [[alpha_s**2, rho*alpha_s*alpha_t], \n",
    "              [rho*alpha_s*alpha_t, alpha_t**2]]\n",
    "\n",
    "lambda_val, nu = 1, 1  # adjust these as needed\n",
    "\n",
    "# Generate omega, beta from multivariate normal\n",
    "omega_beta = multivariate_normal.rvs([1, 1], cov_matrix, size=dim)\n",
    "omega_s, omega_t = omega_beta[:, 0], omega_beta[:, 1]\n",
    "\n",
    "# Generate covariates\n",
    "Z1 = np.random.normal(1.05, 0.0225, (N_s + N_t, 1))\n",
    "Z2 = np.random.normal(30, 5, (N_s + N_t, 1))\n",
    "Z3 = np.random.normal(90, 5, (N_s + N_t, 1))\n",
    "Z4 = bernoulli.rvs(0.5, size=(N_s + N_t, 1))\n",
    "Z5 = bernoulli.rvs(0.5, size=(N_s + N_t, 1))\n",
    "\n",
    "# Scaling only the continuous covariates\n",
    "scaler = StandardScaler()\n",
    "Z_continuous = np.hstack([Z1, Z2, Z3])\n",
    "Z_continuous = scaler.fit_transform(Z_continuous)\n",
    "\n",
    "X = np.hstack([Z_continuous, Z4, Z5])\n",
    "\n",
    "X_s, X_t = X[:N_s], X[N_s:]\n",
    "\n",
    "# Simulate event times\n",
    "U = np.random.uniform(0, 1, N_s)\n",
    "y_s = ((-np.log(U) / (lambda_val * np.exp(X_s.dot(omega_s))))**(1/nu))\n",
    "\n",
    "U = np.random.uniform(0, 1, N_t)\n",
    "y_t = ((-np.log(U) / (lambda_val * np.exp(X_t.dot(omega_t))))**(1/nu))\n",
    "\n",
    "# Censoring\n",
    "lambda_c, nu_c = 2.5, 1  # adjust these as needed\n",
    "C_s = weibull_min.rvs(c=nu_c, scale=lambda_c, size=N_s)\n",
    "C_t = weibull_min.rvs(c=nu_c, scale=lambda_c, size=N_t)\n",
    "\n",
    "y_s_obs = np.minimum(y_s, C_s)\n",
    "y_t_obs = np.minimum(y_t, C_t)\n",
    "\n",
    "delta_s = (y_s <= C_s).astype(int) #delta =1 indicates event \n",
    "delta_t = (y_t <= C_t).astype(int)\n",
    "\n",
    "# Split target data\n",
    "X_t_train, X_t_test, y_t_train_obs, y_t_test_obs, delta_t_train, delta_t_test = train_test_split(\n",
    "    X_t, y_t_obs, delta_t, test_size=0.5, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7e89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already generated X_s, X_t, y_s, and y_t from the previous code...\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_X_s = pd.DataFrame(X_s)\n",
    "df_X_t = pd.DataFrame(X_t)\n",
    "df_y_s = pd.DataFrame(y_s, columns=['y_s'])\n",
    "df_y_t = pd.DataFrame(y_t, columns=['y_t'])\n",
    "\n",
    "# Save to CSV files\n",
    "df_X_s.to_csv('X_s.csv', index=False)\n",
    "df_X_t.to_csv('X_t.csv', index=False)\n",
    "df_y_s.to_csv('y_s.csv', index=False)\n",
    "df_y_t.to_csv('y_t.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84939064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
